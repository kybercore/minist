{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p83dFOSifbYa"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download and load the training and test datasets\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transforms.ToTensor(), download=True)"
      ],
      "metadata": {
        "id": "xDlA3NT6ffTU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "# only the train dataset gets shuffled\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "PxC3LkzLgIcJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MNISTModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(784, 256)\n",
        "    # dropout of 30 % for each step means that the neurons are randomly zeroed\n",
        "    # to force the model to pay attention to overarching themes instead of\n",
        "    # intricacies from a few neurons (overfitting)\n",
        "    self.dropout1 = nn.Dropout(0.3)\n",
        "    self.linear2 = nn.Linear(256, 128)\n",
        "    self.linear3 = nn.Linear(128, 64)\n",
        "    self.linear4 = nn.Linear(64, 10)\n",
        "    # 64 = number of neurons, 10 = number of output neurons\n",
        "    # representative of classes 0 - 9\n",
        "\n",
        "  def forward(self, x):\n",
        "    # reshapes input tensor x into 2d tensor\n",
        "    # (-1 = automatically calculate number of rows)\n",
        "    x = x.view(-1, 784)\n",
        "    x = torch.relu(self.linear1(x)) # activation function\n",
        "    x = self.dropout1(x)\n",
        "    x = torch.relu(self.linear2(x))\n",
        "    x = torch.relu(self.linear3(x))\n",
        "    x = self.linear4(x)\n",
        "    return x\n",
        "\n",
        "model = MNISTModel()"
      ],
      "metadata": {
        "id": "kp4iBIkVgjLy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# loss Function measures how well the model's predctions\n",
        "# match the true labels\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer updates the model's weights during training\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.006)"
      ],
      "metadata": {
        "id": "6wg7LkMbi0M8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 6 # iterations over the entire dataset\n",
        "for epoch in range(num_epochs):\n",
        "  for batch_idx, (data,targets) in enumerate(train_loader):\n",
        "    # forward pass (input images from train_loader are passed through\n",
        "    # the model for predictions)\n",
        "    outputs = model(data)\n",
        "    # loss = difference between predictions and true labels\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # resetting of the gradients so that this batch's weight updates\n",
        "    # from optimizer are not influenced by previous batch's gradients\n",
        "    optimizer.zero_grad()\n",
        "    # backward propagation/pass to calculate gradients\n",
        "    loss.backward()\n",
        "    optimizer.step() # this is the optimization of the weights using gradients\n",
        "\n",
        "    # print statistics\n",
        "    if batch_idx % 100 == 0:\n",
        "        print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0UaH8gmj1ib",
        "outputId": "b27a4d1a-8d63-4baf-9199-8a1feb55e3e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Batch: 1, Loss: 2.3144\n",
            "Epoch: 1, Batch: 101, Loss: 0.3602\n",
            "Epoch: 1, Batch: 201, Loss: 0.3078\n",
            "Epoch: 1, Batch: 301, Loss: 0.4019\n",
            "Epoch: 1, Batch: 401, Loss: 0.2588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "# for eval, loop over test set without updating gradients\n",
        "with torch.no_grad():\n",
        "  for data, targets in test_loader:\n",
        "    # compute loss by comparing predictions with true labels\n",
        "    outputs = model(data)\n",
        "    test_loss += criterion(outputs, targets).item()\n",
        "\n",
        "    # [outputs.data] yields the raw output tensors from model (one tensor\n",
        "    # per input in the batch)\n",
        "    # [torch.max] returns two tensors: 1. the maximum value in each output\n",
        "    # tensor (across 10 class scores), and the index of that maximum value\n",
        "    # (which corresponds to the predicted class label, stored in [predicted])\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    # values essentially represent the confidence in the respectively highest\n",
        "    # confidence class prediction\n",
        "    # the loss function optimizes for the softmax probability of te true class\n",
        "    # to be as high as possible, and the 9 other classes' to be as low as\n",
        "    # possible\n",
        "    # print(torch.max(outputs.data, 1))\n",
        "    correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "acc = 100 * correct / len(test_loader.dataset)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {acc:.2f}%')"
      ],
      "metadata": {
        "id": "vynHVLQKlyCe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}